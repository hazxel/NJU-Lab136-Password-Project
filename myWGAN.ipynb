{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from data_prep import Password as P\n",
    "from model import Generator, Discriminator\n",
    "from training_helper import *\n",
    "from config import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on G and D\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator(GEN_HIDDEN_SIZE, GEN_NEURON_SIZE).to(device)\n",
    "d = Discriminator(DISC_HIDDEN_SIZE, DISC_NEURON_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_g = torch.optim.Adam(g.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#opt_d = torch.optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_g = torch.optim.RMSprop(g.parameters(), lr=0.0002)\n",
    "opt_d = torch.optim.RMSprop(d.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = P()\n",
    "batch_gen = p.string_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "TRAIN_FROM_CKPT = True\n",
    "\n",
    "if os.path.isfile(CHECKPOINT_PATH) and TRAIN_FROM_CKPT:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location = device)\n",
    "    g.load_state_dict(checkpoint['gen_state_dict'])\n",
    "    d.load_state_dict(checkpoint['disc_state_dict'])\n",
    "    g = g.to(device)\n",
    "    d = d.to(device)\n",
    "    opt_g.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n",
    "    opt_d.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n",
    "    start_len = checkpoint['seq_len']\n",
    "    start_iter = checkpoint['iter']\n",
    "    disc_loss = checkpoint['disc_loss']\n",
    "    gen_loss = checkpoint['gen_loss']\n",
    "else:\n",
    "    start_len = 1\n",
    "    start_iter = 1\n",
    "    disc_loss = []\n",
    "    gen_loss = []\n",
    "\n",
    "for seq_len in range(start_len, MAX_LEN + 1):\n",
    "    logging.info(\"---------- Adversarial Training with Seq Len %d, Batch Size %d ----------\\n\" \n",
    "                 % (seq_len, BATCH_SIZE))\n",
    "    \n",
    "    for i in range(start_iter, ITERS_PER_SEQ_LEN + 1):\n",
    "                \n",
    "        if i % SAVE_CHECKPOINTS_EVERY == 0:\n",
    "            torch.save({\n",
    "                'seq_len': seq_len,\n",
    "                'gen_state_dict': g.state_dict(),\n",
    "                'disc_state_dict': d.state_dict(),\n",
    "                'gen_optimizer_state_dict': opt_g.state_dict(),\n",
    "                'disc_optimizer_state_dict': opt_d.state_dict(),\n",
    "                'iter': i,\n",
    "                'gen_loss': gen_loss,\n",
    "                'disc_loss': disc_loss\n",
    "                }, CHECKPOINT_PATH)\n",
    "            logging.info(\"  *** Model Saved ***\\n\")\n",
    "        \n",
    "        logging.debug(\"----------------- %d / %d -----------------\\n\" % (i, ITERS_PER_SEQ_LEN))\n",
    "\n",
    "        logging.debug(\"Training discriminator...\\n\")\n",
    "        \n",
    "        d.requiresGrad()\n",
    "        d.zero_grad()\n",
    "        g.zero_grad()\n",
    "        for j in range(CRITIC_ITERS):\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                L = 0\n",
    "                \n",
    "                data = next(batch_gen)\n",
    "                pred = g(data, seq_len)\n",
    "                real, fake = get_train_dis(data, pred, seq_len)\n",
    "                interpolate = get_interpolate(real, fake)\n",
    "\n",
    "                # Genuine\n",
    "                disc_real = d(real, seq_len)\n",
    "                loss_real = -disc_real.mean()\n",
    "                logging.debug(\"real loss: \"+str(loss_real.item()))\n",
    "                L += loss_real\n",
    "\n",
    "                # Fake\n",
    "                disc_fake = d(fake, seq_len)\n",
    "                loss_fake = disc_fake.mean()\n",
    "                logging.debug(\"fake loss: \"+str(loss_fake.item()))\n",
    "                L += loss_fake\n",
    "\n",
    "                # Gradient penalty\n",
    "                interpolate = torch.autograd.Variable(interpolate, requires_grad=True)\n",
    "                disc_interpolate = d(interpolate, seq_len)\n",
    "                grad = torch.ones_like(disc_interpolate).to(device)\n",
    "                gradients = torch.autograd.grad(\n",
    "                        outputs=disc_interpolate,\n",
    "                        inputs=interpolate,\n",
    "                        grad_outputs=grad,\n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,\n",
    "                        only_inputs=True,\n",
    "                    )[0]\n",
    "                loss_gp = ((gradients.norm(2, dim=2) - 1) ** 2).mean() * LAMBDA\n",
    "                logging.debug(\"grad loss: \"+str(loss_gp.item()))\n",
    "                L += loss_gp\n",
    "\n",
    "                L.backward(retain_graph=False)\n",
    "                opt_d.step()\n",
    "                \n",
    "                logging.debug(\"Critic Iter \" + str(j+1) + \" Loss: \" + str(L.item()) + \"\\n\")\n",
    "\n",
    "\n",
    "        logging.debug(\"Done training discriminator.\\n\")    \n",
    "\n",
    "        logging.debug(\"Training generator...\")\n",
    "\n",
    "        d.requiresNoGrad()\n",
    "\n",
    "        for j in range(GEN_ITERS):\n",
    "            data = next(batch_gen)\n",
    "            pred = g(data, seq_len)\n",
    "            fake = get_train_gen(data, pred, seq_len)\n",
    "            loss_gen = -d(fake, seq_len).mean()\n",
    "            logging.debug(\"Gen Iter \" + str(j+1) + \" Loss: \"+str(loss_gen.item()))\n",
    "            loss_gen.backward(retain_graph=False)\n",
    "            opt_g.step()\n",
    "\n",
    "        logging.debug(\"Done training generator.\\n\")\n",
    "\n",
    "        if i % SAVE_CHECKPOINTS_EVERY == 0:\n",
    "            disc_loss.append(L)\n",
    "            gen_loss.append(loss_gen)\n",
    "    \n",
    "    start_iter = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_trend)\n",
    "#plt.savefig('image/rnn-rnn-loss.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "loss_trend = []\n",
    "logging.info(\"---------- Pre-training generator ----------\")\n",
    "for i in range(PRE_GEN_ITERS):\n",
    "    pas = next(batch_gen)\n",
    "    input_tensor = P.passwordToInputTensor(pas).to(device)\n",
    "    target_tensor = P.passwordToTargetTensor(pas).to(device)\n",
    "    output, loss = g.pre_train(input_tensor, target_tensor)\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        loss_trend.append(loss)\n",
    "        logging.debug(\"Iter: \"+ str(i)+\" Loss: \"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 1]), 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr = nn.GRU(5, 6, 1, batch_first = True, dropout = 0).to(device)\n",
    "inp = torch.ones(2,3,5).to(device)\n",
    "print(inp.dim())\n",
    "le = [2,3]\n",
    "inp = nn.utils.rnn.pack_padded_sequence(inp, le, batch_first=True, enforce_sorted=False)\n",
    "ii, batch_sizes, sorted_indices, unsorted_indices = inp\n",
    "output = gr(inp)\n",
    "batch_sizes, ii.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location = device)\n",
    "g.load_state_dict(checkpoint['gen_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pre_train(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f9d9a1ec5a1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\NJU_files\\I_Password\\NJU-Lab136-Password-Project\\model.py\u001b[0m in \u001b[0;36mpre_train\u001b[1;34m(self, p, G)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfake_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "d.pre_train(p, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gcccccccccccccccccc', '0202020202020202020', '1202020202020202020', 'epes8', 'ia10202020202020202', 's8alalalalalalalala', 'piiiiiiiiiiiiiiiiii', 'bypepes8', '4iia020202020202020', 'piiamiamiamiamames8', 'es8', '1202020202020202020', 'bypepees8', '0020200200200200200', 'piiiames8', 'kia0000000000000000', '9720202020202020202', '1202020202020202020', '0202020202020202020', 'piiames8']\n"
     ]
    }
   ],
   "source": [
    "print(g.generate_N(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.0,2],[3,4]]).to(device)\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
