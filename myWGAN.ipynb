{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from data_prep import Password as P\n",
    "from model import Generator, Discriminator\n",
    "from training_helper import *\n",
    "from config import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on G and D\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator(GEN_HIDDEN_SIZE, GEN_NEURON_SIZE).to(device)\n",
    "d = Discriminator(DISC_HIDDEN_SIZE, DISC_NEURON_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_g = torch.optim.Adam(g.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#opt_d = torch.optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_g = torch.optim.RMSprop(g.parameters(), lr=0.0002)\n",
    "opt_d = torch.optim.RMSprop(d.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing passwords...\n",
      "DEBUG:root:Loading from existing json file...\n",
      "INFO:root:Done initializing passwords.\n"
     ]
    }
   ],
   "source": [
    "p = P()\n",
    "batch_gen = p.string_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "TRAIN_FROM_CKPT = True\n",
    "\n",
    "if os.path.isfile(CHECKPOINT_PATH) and TRAIN_FROM_CKPT:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location = device)\n",
    "    g.load_state_dict(checkpoint['gen_state_dict'])\n",
    "    d.load_state_dict(checkpoint['disc_state_dict'])\n",
    "    g = g.to(device)\n",
    "    d = d.to(device)\n",
    "    opt_g.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n",
    "    opt_d.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n",
    "    start_len = checkpoint['seq_len']\n",
    "    start_iter = checkpoint['iter']\n",
    "    disc_loss = checkpoint['disc_loss']\n",
    "    gen_loss = checkpoint['gen_loss']\n",
    "else:\n",
    "    start_len = 1\n",
    "    start_iter = 1\n",
    "    disc_loss = []\n",
    "    gen_loss = []\n",
    "\n",
    "for seq_len in range(start_len, MAX_LEN + 1):\n",
    "    logging.info(\"---------- Adversarial Training with Seq Len %d, Batch Size %d ----------\\n\" \n",
    "                 % (seq_len, BATCH_SIZE))\n",
    "    \n",
    "    for i in range(start_iter, ITERS_PER_SEQ_LEN + 1):\n",
    "                \n",
    "        if i % SAVE_CHECKPOINTS_EVERY == 0:\n",
    "            torch.save({\n",
    "                'seq_len': seq_len,\n",
    "                'gen_state_dict': g.state_dict(),\n",
    "                'disc_state_dict': d.state_dict(),\n",
    "                'gen_optimizer_state_dict': opt_g.state_dict(),\n",
    "                'disc_optimizer_state_dict': opt_d.state_dict(),\n",
    "                'iter': i,\n",
    "                'gen_loss': gen_loss,\n",
    "                'disc_loss': disc_loss\n",
    "                }, CHECKPOINT_PATH)\n",
    "            logging.info(\"  *** Model Saved ***\\n\")\n",
    "        \n",
    "        logging.debug(\"----------------- %d / %d -----------------\\n\" % (i, ITERS_PER_SEQ_LEN))\n",
    "\n",
    "        logging.debug(\"Training discriminator...\\n\")\n",
    "        \n",
    "        d.requiresGrad()\n",
    "        d.zero_grad()\n",
    "        g.zero_grad()\n",
    "        for j in range(CRITIC_ITERS):\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                L = 0\n",
    "                \n",
    "                data = next(batch_gen)\n",
    "                pred = g(data, seq_len)\n",
    "                real, fake = get_train_dis(data, pred, seq_len)\n",
    "                interpolate = get_interpolate(real, fake)\n",
    "\n",
    "                # Genuine\n",
    "                disc_real = d(real, seq_len)\n",
    "                loss_real = -disc_real.mean()\n",
    "                logging.debug(\"real loss: \"+str(loss_real.item()))\n",
    "                L += loss_real\n",
    "\n",
    "                # Fake\n",
    "                disc_fake = d(fake, seq_len)\n",
    "                loss_fake = disc_fake.mean()\n",
    "                logging.debug(\"fake loss: \"+str(loss_fake.item()))\n",
    "                L += loss_fake\n",
    "\n",
    "                # Gradient penalty\n",
    "                interpolate = torch.autograd.Variable(interpolate, requires_grad=True)\n",
    "                disc_interpolate = d(interpolate, seq_len)\n",
    "                grad = torch.ones_like(disc_interpolate).to(device)\n",
    "                gradients = torch.autograd.grad(\n",
    "                        outputs=disc_interpolate,\n",
    "                        inputs=interpolate,\n",
    "                        grad_outputs=grad,\n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,\n",
    "                        only_inputs=True,\n",
    "                    )[0]\n",
    "                loss_gp = ((gradients.norm(2, dim=2) - 1) ** 2).mean() * LAMBDA\n",
    "                logging.debug(\"grad loss: \"+str(loss_gp.item()))\n",
    "                L += loss_gp\n",
    "\n",
    "                L.backward(retain_graph=False)\n",
    "                opt_d.step()\n",
    "                \n",
    "                logging.debug(\"Critic Iter \" + str(j+1) + \" Loss: \" + str(L.item()) + \"\\n\")\n",
    "\n",
    "\n",
    "        logging.debug(\"Done training discriminator.\\n\")    \n",
    "\n",
    "        logging.debug(\"Training generator...\")\n",
    "\n",
    "        d.requiresNoGrad()\n",
    "\n",
    "        for j in range(GEN_ITERS):\n",
    "            data = next(batch_gen)\n",
    "            pred = g(data, seq_len)\n",
    "            fake = get_train_gen(data, pred, seq_len)\n",
    "            loss_gen = -d(fake, seq_len).mean()\n",
    "            logging.debug(\"Gen Iter \" + str(j+1) + \" Loss: \"+str(loss_gen.item()))\n",
    "            loss_gen.backward(retain_graph=False)\n",
    "            opt_g.step()\n",
    "\n",
    "        logging.debug(\"Done training generator.\\n\")\n",
    "\n",
    "        if i % SAVE_CHECKPOINTS_EVERY == 0:\n",
    "            disc_loss.append(L)\n",
    "            gen_loss.append(loss_gen)\n",
    "    \n",
    "    start_iter = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_trend)\n",
    "#plt.savefig('image/rnn-rnn-loss.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "loss_trend = []\n",
    "logging.info(\"---------- Pre-training generator ----------\")\n",
    "for i in range(PRE_GEN_ITERS):\n",
    "    pas = next(batch_gen)\n",
    "    input_tensor = P.passwordToInputTensor(pas).to(device)\n",
    "    target_tensor = P.passwordToTargetTensor(pas).to(device)\n",
    "    output, loss = g.pre_train(input_tensor, target_tensor)\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        loss_trend.append(loss)\n",
    "        logging.debug(\"Iter: \"+ str(i)+\" Loss: \"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 1]), 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr = nn.GRU(5, 6, 1, batch_first = True, dropout = 0).to(device)\n",
    "inp = torch.ones(2,3,5).to(device)\n",
    "print(inp.dim())\n",
    "le = [2,3]\n",
    "inp = nn.utils.rnn.pack_padded_sequence(inp, le, batch_first=True, enforce_sorted=False)\n",
    "ii, batch_sizes, sorted_indices, unsorted_indices = inp\n",
    "output = gr(inp)\n",
    "batch_sizes, ii.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location = device)\n",
    "g.load_state_dict(checkpoint['gen_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f9ff708bf34a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\NJU_files\\I_Password\\NJU-Lab136-Password-Project\\model.py\u001b[0m in \u001b[0;36mpre_train\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpassword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpasswords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mpassword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpasswords_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mletterToIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHARMAP_LEN\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "g.pre_train(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r722222222222222222', 'bisbb12222222222222', 'sabbbbbbbbbbbbbbbbb', '4ATTIE4444444444444', 'gabbbbbbbbbbbbbbbbb', '92abbbbbbbbbbbbbbbb', 'mayfair000000000000', 'mayair3', 'rabbbbbbbbbbbbbbbbb', '1222222222222222222', 'tpit', '50pir', 'pisabbbbbbbbbbbbbbb', 'sbbbbbbbbbbbbbbbbbb', '92222abbbbbbbbbbbbb', 'ci4u222222222222222', 'jkw3bb0pir', 'nedi3', 'o222222222222222222', 'li4u222222222222222']\n"
     ]
    }
   ],
   "source": [
    "print(g.generate_N(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9048, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([4]).to(device)\n",
    "b = torch.tensor([[0.0,0,0,0,1]]).to(device)\n",
    "c = nn.CrossEntropyLoss()\n",
    "print(c(b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
